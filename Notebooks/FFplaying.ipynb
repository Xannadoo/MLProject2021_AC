{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = '../Data/df_train.csv'\n",
    "TEST = '../Data/df_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from exercise 5 to calculate the z-score\n",
    "z_score = lambda x : (x - np.mean(x, axis=0)) / np.std(x, axis=0)\n",
    "\n",
    "def conf_mat(y_hat, y_true):\n",
    "    '''Returns a confusion matrix'''\n",
    "    n = max(y_hat)+1\n",
    "    bingo = np.zeros([n,n])\n",
    "    for i in range(len(y_hat)):\n",
    "        bingo[y_true[i]][y_hat[i]] +=1\n",
    "    \n",
    "    return(bingo)\n",
    "\n",
    "def scores(y_hat, y_true, average = True):\n",
    "    '''For each class, returns recall, precision and f1'''\n",
    "    classes = list(np.unique(y_true))\n",
    "    conf = conf_mat(y_hat, y_true)\n",
    "    r = []\n",
    "    p = []\n",
    "    f = []\n",
    "    for c in classes:\n",
    "        recall = conf[c][c] / sum(conf[c])\n",
    "        precision = conf[c][c] / sum(conf[:, c])\n",
    "        f1 = 2*(precision*recall)/(precision + recall)\n",
    "        r.append(recall)\n",
    "        p.append(precision)\n",
    "        f.append(f1)\n",
    "    if average:\n",
    "        return sum(r)/len(r), sum(p)/len(p), sum(f)/len(f)\n",
    "    else:\n",
    "        return (r, p, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables and df loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'rainbow' # Colour theme\n",
    "\n",
    "df = pd.read_csv(TRAIN) # Training dataframe\n",
    "a = len(df)\n",
    "\n",
    "#ensures data is without order, random state fixed for reproducability, frac=1 gives the whole df back but shuffled\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "if a != len(df):\n",
    "    print('WARNING, DATA IS BEING LOST')#confirm still have the whole df\n",
    "\n",
    "attributes = list(df.columns)[:-1] # Creates list of column names for the dataframe without the class\n",
    "\n",
    "df[attributes] = z_score(df[attributes])\n",
    "\n",
    "X = df[attributes].copy() # Attributes\n",
    "y = df['type'].copy() # True values\n",
    "\n",
    "for i in range(len(y)):\n",
    "    if y[i] > 4:\n",
    "        y[i] = y[i] - 2\n",
    "    else:\n",
    "        y[i] = y[i] - 1\n",
    "        \n",
    "lb = preprocessing.LabelBinarizer()\n",
    "new_y = pd.DataFrame(lb.fit_transform(y))\n",
    "\n",
    "y_list = y.unique() # 'y' values\n",
    "\n",
    "#round(df.describe(),2)\n",
    "df['type'] = y\n",
    "df[[0,1,2,3,4,5]] = new_y\n",
    "\n",
    "df.head(10)\n",
    "round(df[attributes].describe(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ideas from:\n",
    "\n",
    "https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6\n",
    "\n",
    "https://hackernoon.com/building-a-feedforward-neural-network-from-scratch-in-python-d3526457156b\n",
    "\n",
    "https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, x, y, bias=None, neurons=12):\n",
    "        '''\n",
    "        neurons --> neurons per hidden layer\n",
    "        activation --> choose activation function\n",
    "        '''\n",
    "        \n",
    "        self.input = x # All rows with the attributes (all X's)\n",
    "        #self.y = np.array(y) # True values\n",
    "        self.labels = y\n",
    "        self.rows = x.shape[0]\n",
    "        self.class_count = len(y.unique())\n",
    "        self.y = np.zeros((self.rows, self.class_count))\n",
    "        for i in range(self.rows):\n",
    "            self.y[i, self.labels[i]] = 1\n",
    "        \n",
    "        self.output = np.zeros(self.y.shape) #\n",
    "        \n",
    "        self.neurons = neurons\n",
    "        np.random.seed(23)\n",
    "        self.weight1 = np.random.rand(x.shape[1], neurons) # (attributes in X, number of neurons)\n",
    "        #self.weight2 = np.random.rand(neurons, neurons)\n",
    "        self.weight_final = np.random.rand(neurons, self.y.shape[1])\n",
    "\n",
    "        self.bias1 = bias[0]\n",
    "        #self.bias2 = bias[1]\n",
    "        self.bias_final = bias[1]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def tanh(self):\n",
    "        return np.tanh(self.x)\n",
    "\n",
    "    def relu(self): #Rectified Linear Unit\n",
    "        return np.maximum(0, self.x)\n",
    "    \n",
    "    def softmax(self, vec):\n",
    "        #print(f'type vec: {type(vec)}')\n",
    "        exp = np.exp(vec.to_numpy())\n",
    "        probabilities = exp / np.sum(exp,axis=1, keepdims=True)\n",
    "        return probabilities \n",
    "    \n",
    "    # https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6\n",
    "    # code borrowed from here\n",
    "    def sigmoid_derivative(self, p):\n",
    "        return p * (1 - p)\n",
    "       \n",
    "    def forwardpass(self, x):\n",
    "        # activation_function(x * w_0 + bias_0) etc\n",
    "        \n",
    "        # Going through the 1st layer\n",
    "        self.layer1 = self.sigmoid((x.dot(self.weight1) + self.bias1))\n",
    "        \n",
    "        #self.layer2 = self.sigmoid(self.layer1.dot(self.weight2) + self.bias2)\n",
    "        #print(self.layer2.shape)\n",
    "        \n",
    "        # Final Output layer with softmax\n",
    "        self.output = self.softmax(self.layer1.dot(self.weight_final) + self.bias_final)\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def categorical_cross_entropy(self, X):\n",
    "        '''Function to calculate categorical cross entropy\n",
    "        '''\n",
    "        p = self.output # predictions\n",
    "        sum_score = 0.0\n",
    "        for i in range(len(self.y)):\n",
    "            for j in range(len(self.y[i])):\n",
    "                sum_score += self.y[i][j] * np.log(1e-15 + p[i][j])\n",
    "        mean_sum_score = 1.0 / len(self.y) * sum_score\n",
    "        return -mean_sum_score\n",
    "    \n",
    "    def backprop(self):\n",
    "        # Update weights of last layer \n",
    "        d_weights2 = np.dot(self.layer1.T, (2*(self.output - self.y) * self.categorical_cross_entropy(self.output)))\n",
    "        \n",
    "        # Update weights of 1st layer\n",
    "        d_weights1 = np.dot(self.input.T, np.dot(2*(self.y - self.output) * self.sigmoid_derivative(self.output), self.weight_final.T)*self.sigmoid_derivative(self.layer1))\n",
    "        \n",
    "        #print(self.input.shape, self.weight1.shape, self.y.shape, self.output.shape, self.weight_final.shape, self.layer1.shape)\n",
    "\n",
    "        # Update weights\n",
    "        self.weight1 += d_weights1\n",
    "        self.weight_final += d_weights2\n",
    "\n",
    "    def train(self, X, y):\n",
    "        self.output = self.forwardpass(X)\n",
    "        self.backprop()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias=np.array([0]*3)\n",
    "\n",
    "kitty = NeuralNetwork(X, y, bias = bias)\n",
    "#print(kitty.weight1.shape, kitty.weight2.shape)\n",
    "#kitty.forwardpass(X)\n",
    "#kitty.backprop()\n",
    "for i in range(10):\n",
    "    kitty.train(X, new_y)\n",
    "    print(sum(kitty.output[0]), kitty.output[0], np.argmax(kitty.output[0]), y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_der(x):\n",
    "    return sigmoid(x) *(1-sigmoid (x))\n",
    "\n",
    "def softmax(A):\n",
    "    expA = np.exp(A)\n",
    "    return expA / expA.sum(axis=1, keepdims=True)\n",
    "\n",
    "instances = X.shape[0]\n",
    "attributes = X.shape[1]\n",
    "hidden_nodes = 4\n",
    "output_labels = 6\n",
    "\n",
    "weight1 = np.random.rand(attributes, hidden_nodes)\n",
    "bias1 = np.random.randn(hidden_nodes)\n",
    "\n",
    "weight_final = np.random.rand(hidden_nodes, output_labels)\n",
    "bias_final = np.random.randn(output_labels)\n",
    "learning_rate = 0.01\n",
    "\n",
    "one_hot_labels = np.zeros((149, 6))\n",
    "for i in range(149):\n",
    "    one_hot_labels[i, y[i]] = 1\n",
    "\n",
    "error_cost = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "############# feedforward\n",
    "\n",
    "    # Phase 1\n",
    "    layer1 = sigmoid(np.dot(X, weight1) + bias1)\n",
    "\n",
    "    # Phase 2\n",
    "    output = softmax(np.dot(layer1, weight_final) + bias_final)\n",
    "\n",
    "########## Back Propagation\n",
    "########## Phase 1\n",
    "\n",
    "    difference = output - one_hot_labels\n",
    "\n",
    "    weight_cost = np.dot(layer1.T, difference)\n",
    "\n",
    "    bias_cost = difference\n",
    "\n",
    "########## Phases 2\n",
    "\n",
    "    dzo_dah = weight_final\n",
    "    dcost_dah = np.dot(difference , weight_cost.T)\n",
    "    dah_dzh = sigmoid_der(np.dot(X, weight1) + bias1)\n",
    "    dzh_dwh = X\n",
    "    dcost_wh = np.dot(X.T, sigmoid_der(np.dot(X, weight1) + bias1) * np.dot(difference , weight_cost.T))\n",
    "\n",
    "    dcost_bh = np.dot(difference , weight_cost.T) * sigmoid_der(np.dot(X, weight1) + bias1)\n",
    "\n",
    "    # Update Weights ================\n",
    "\n",
    "    weight1 -= learning_rate * dcost_wh\n",
    "    bias1 -= learning_rate * dcost_bh.sum(axis=0)\n",
    "\n",
    "    weight_final -= learning_rate * weight_cost\n",
    "    bias_final -= learning_rate * bias_cost.sum(axis=0)\n",
    "\n",
    "    loss = np.sum(-one_hot_labels * np.log(output))\n",
    "    #print('Loss function value: ', loss)\n",
    "    error_cost.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(error_cost)\n",
    "plt.ylim(150,250);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
