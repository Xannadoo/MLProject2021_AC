{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = '../Data/df_train.csv'\n",
    "TEST = '../Data/df_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from exercise 5 to calculate the z-score\n",
    "z_score = lambda x : (x - np.mean(x, axis=0)) / np.std(x, axis=0)\n",
    "\n",
    "def conf_mat(y_hat, y_true):\n",
    "    '''Returns a confusion matrix'''\n",
    "    n = max(y_hat)+1\n",
    "    bingo = np.zeros([n,n])\n",
    "    for i in range(len(y_hat)):\n",
    "        bingo[y_true[i]][y_hat[i]] +=1\n",
    "    \n",
    "    return(bingo)\n",
    "\n",
    "def scores(y_hat, y_true, average = True):\n",
    "    '''For each class, returns recall, precision and f1'''\n",
    "    classes = list(np.unique(y_true))\n",
    "    conf = conf_mat(y_hat, y_true)\n",
    "    r = []\n",
    "    p = []\n",
    "    f = []\n",
    "    for c in classes:\n",
    "        recall = conf[c][c] / sum(conf[c])\n",
    "        precision = conf[c][c] / sum(conf[:, c])\n",
    "        f1 = 2*(precision*recall)/(precision + recall)\n",
    "        r.append(recall)\n",
    "        p.append(precision)\n",
    "        f.append(f1)\n",
    "    if average:\n",
    "        return sum(r)/len(r), sum(p)/len(p), sum(f)/len(f)\n",
    "    else:\n",
    "        return (r, p, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables and df loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>149.00</td>\n",
       "      <td>149.00</td>\n",
       "      <td>149.00</td>\n",
       "      <td>149.00</td>\n",
       "      <td>149.00</td>\n",
       "      <td>149.00</td>\n",
       "      <td>149.00</td>\n",
       "      <td>149.00</td>\n",
       "      <td>149.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.27</td>\n",
       "      <td>-3.13</td>\n",
       "      <td>-1.92</td>\n",
       "      <td>-2.27</td>\n",
       "      <td>-3.61</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-2.32</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>-0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>-0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>-0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.84</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.89</td>\n",
       "      <td>4.09</td>\n",
       "      <td>3.57</td>\n",
       "      <td>10.08</td>\n",
       "      <td>4.82</td>\n",
       "      <td>5.35</td>\n",
       "      <td>3.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           RI      Na      Mg      Al      Si       K      Ca      Ba      Fe\n",
       "count  149.00  149.00  149.00  149.00  149.00  149.00  149.00  149.00  149.00\n",
       "mean     0.00   -0.00    0.00   -0.00   -0.00    0.00    0.00    0.00    0.00\n",
       "std      1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00\n",
       "min     -2.27   -3.13   -1.92   -2.27   -3.61   -0.85   -2.32   -0.36   -0.63\n",
       "25%     -0.56   -0.57   -0.31   -0.52   -0.44   -0.63   -0.47   -0.36   -0.63\n",
       "50%     -0.23   -0.14    0.54   -0.15    0.20    0.11   -0.22   -0.36   -0.63\n",
       "75%      0.23    0.48    0.62    0.37    0.55    0.22    0.14   -0.36    0.50\n",
       "max      4.84    4.60    0.89    4.09    3.57   10.08    4.82    5.35    3.17"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = 'rainbow' # Colour theme\n",
    "\n",
    "df = pd.read_csv(TRAIN) # Training dataframe\n",
    "a = len(df)\n",
    "\n",
    "#ensures data is without order, random state fixed for reproducability, frac=1 gives the whole df back but shuffled\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "if a != len(df):\n",
    "    print('WARNING, DATA IS BEING LOST')#confirm still have the whole df\n",
    "\n",
    "attributes = list(df.columns)[:-1] # Creates list of column names for the dataframe without the class\n",
    "\n",
    "df[attributes] = z_score(df[attributes])\n",
    "\n",
    "X = df[attributes].copy() # Attributes\n",
    "y = df['type'].copy() # True values\n",
    "\n",
    "for i in range(len(y)):\n",
    "    if y[i] > 4:\n",
    "        y[i] = y[i] - 2\n",
    "    else:\n",
    "        y[i] = y[i] - 1\n",
    "        \n",
    "lb = preprocessing.LabelBinarizer()\n",
    "new_y = pd.DataFrame(lb.fit_transform(y))\n",
    "\n",
    "y_list = y.unique() # 'y' values\n",
    "\n",
    "#round(df.describe(),2)\n",
    "df['type'] = y\n",
    "df[[0,1,2,3,4,5]] = new_y\n",
    "\n",
    "df.head(10)\n",
    "round(df[attributes].describe(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ideas from:\n",
    "\n",
    "https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6\n",
    "\n",
    "https://hackernoon.com/building-a-feedforward-neural-network-from-scratch-in-python-d3526457156b\n",
    "\n",
    "https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, x, y, neurons=12, lr=0.01):\n",
    "        '''\n",
    "        neurons --> neurons per hidden layer\n",
    "        activation --> choose activation function\n",
    "        '''\n",
    "        \n",
    "        self.input = x # All rows with the attributes (all X's)\n",
    "        #self.y = np.array(y) # True values\n",
    "        self.labels = y # true labels\n",
    "        self.rows = x.shape[0] # number of rows\n",
    "        self.class_count = len(y.unique()) # number of classes\n",
    "        self.y = np.zeros((self.rows, self.class_count)) # one hot labels\n",
    "        for i in range(self.rows):\n",
    "            self.y[i, self.labels[i]] = 1\n",
    "        \n",
    "        self.output = np.zeros(self.y.shape) #\n",
    "        \n",
    "        self.neurons = neurons # number of neurons per label\n",
    "        np.random.seed(23)\n",
    "        \n",
    "        self.weight1 = np.random.rand(x.shape[1], neurons) # (attributes in X, number of neurons)\n",
    "        #self.weight2 = np.random.rand(neurons, neurons)\n",
    "        self.weight_final = np.random.rand(neurons, self.y.shape[1])\n",
    "\n",
    "        self.bias1 = np.random.randn(neurons)\n",
    "        #self.bias2 = bias[1]\n",
    "        self.bias_final = np.random.randn(self.class_count)\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.error_cost = []\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def tanh(self):\n",
    "        return np.tanh(self.x)\n",
    "\n",
    "    def relu(self): #Rectified Linear Unit\n",
    "        return np.maximum(0, self.x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp = np.exp(x.to_numpy())\n",
    "        probabilities = exp / np.sum(exp,axis=1, keepdims=True)\n",
    "        return probabilities \n",
    "    \n",
    "    def sigmoid_der(x):\n",
    "        return sigmoid(x) *(1-sigmoid (x))\n",
    "    # https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6\n",
    "    # code borrowed from here\n",
    "       \n",
    "    def forwardpass(self, x):\n",
    "        # activation_function(x * w_0 + bias_0) etc\n",
    "        \n",
    "        # Going through the 1st layer\n",
    "        self.layer1 = self.sigmoid((x.dot(self.weight1) + self.bias1))\n",
    "        \n",
    "        #self.layer2 = self.sigmoid(self.layer1.dot(self.weight2) + self.bias2)\n",
    "        #print(self.layer2.shape)\n",
    "        \n",
    "        # Final Output layer with softmax\n",
    "        self.output = self.softmax(self.layer1.dot(self.weight_final) + self.bias_final)\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def categorical_cross_entropy(self, X):\n",
    "        '''Function to calculate categorical cross entropy\n",
    "        '''\n",
    "        p = self.output # predictions\n",
    "        sum_score = 0.0\n",
    "        for i in range(len(self.y)):\n",
    "            for j in range(len(self.y[i])):\n",
    "                sum_score += self.y[i][j] * np.log(1e-15 + p[i][j])\n",
    "        mean_sum_score = 1.0 / len(self.y) * sum_score\n",
    "        return -mean_sum_score\n",
    "    \n",
    "    def backprop(self):\n",
    "        \"\"\"Back Propogation!!!!\n",
    "        \"\"\"\n",
    "        difference = self.output - self.y # Difference between forward output and true labels\n",
    "        weight_cost = np.dot(self.layer1.T, difference)\n",
    "        \n",
    "        bias_cost = difference\n",
    "        \n",
    "        w = self.weight_final\n",
    "        dcost = np.dot(difference, weight_cost.T)\n",
    "        backwards_sig = sigmoid_der(np.dot(self.input, self.weight1) + self.bias1)\n",
    "        \n",
    "        back_cost_w = np.dot(self.input.T, backwards_sig * np.dot(difference, weight_cost.T))\n",
    "        back_cost_b = np.dot(difference, weight_cost.T) * backwards_sig\n",
    "        \n",
    "        # Update weights and biases using learning rate\n",
    "        self.weight1 -= self.lr * back_cost_w\n",
    "        self.bias1 -= self.lr * back_cost_b.sum(axis=0)\n",
    "        \n",
    "        self.weight_final -= self.lr * weight_cost\n",
    "        self.bias_final -= self.lr * bias_cost.sum(axis=0)\n",
    "        \n",
    "        loss = np.sum(-self.y * np.log(self.output)) # categorical cross entropy\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        self.output = self.forwardpass(X)\n",
    "        self.error_cost.append(self.backprop())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sigmoid_der' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-fda528488cf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#kitty.backprop()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mkitty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkitty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkitty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkitty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-c61dfcce5ea4>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforwardpass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_cost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-c61dfcce5ea4>\u001b[0m in \u001b[0;36mbackprop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_final\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mdcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdifference\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_cost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mbackwards_sig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid_der\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mback_cost_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackwards_sig\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdifference\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_cost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sigmoid_der' is not defined"
     ]
    }
   ],
   "source": [
    "bias=np.array([0]*3)\n",
    "\n",
    "kitty = NeuralNetwork(X, y)\n",
    "#print(kitty.weight1.shape, kitty.weight2.shape)\n",
    "#kitty.forwardpass(X)\n",
    "#kitty.backprop()\n",
    "for i in range(1000):\n",
    "    kitty.train(X, new_y)\n",
    "print(sum(kitty.output[0]), kitty.output[0], np.argmax(kitty.output[0]), y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(kitty.error_cost)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_der(x):\n",
    "    return sigmoid(x) *(1-sigmoid (x))\n",
    "\n",
    "def softmax(A):\n",
    "    expA = np.exp(A)\n",
    "    return expA / expA.sum(axis=1, keepdims=True)\n",
    "\n",
    "instances = X.shape[0]\n",
    "attributes = X.shape[1]\n",
    "hidden_nodes = 4\n",
    "output_labels = 6\n",
    "\n",
    "weight1 = np.random.rand(attributes, hidden_nodes)\n",
    "bias1 = np.random.randn(hidden_nodes)\n",
    "\n",
    "weight_final = np.random.rand(hidden_nodes, output_labels)\n",
    "bias_final = np.random.randn(output_labels)\n",
    "learning_rate = 0.01\n",
    "\n",
    "one_hot_labels = np.zeros((149, 6))\n",
    "for i in range(149):\n",
    "    one_hot_labels[i, y[i]] = 1\n",
    "\n",
    "error_cost = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "############# feedforward\n",
    "\n",
    "    # Phase 1\n",
    "    layer1 = sigmoid(np.dot(X, weight1) + bias1)\n",
    "\n",
    "    # Phase 2\n",
    "    output = softmax(np.dot(layer1, weight_final) + bias_final)\n",
    "\n",
    "########## Back Propagation\n",
    "########## Phase 1\n",
    "\n",
    "    difference = output - one_hot_labels\n",
    "\n",
    "    weight_cost = np.dot(layer1.T, difference)\n",
    "\n",
    "    bias_cost = difference\n",
    "\n",
    "########## Phases 2\n",
    "\n",
    "    dzo_dah = weight_final\n",
    "    dcost_dah = np.dot(difference , weight_cost.T)\n",
    "    dah_dzh = sigmoid_der(np.dot(X, weight1) + bias1)\n",
    "    dzh_dwh = X\n",
    "    dcost_wh = np.dot(X.T, sigmoid_der(np.dot(X, weight1) + bias1) * np.dot(difference , weight_cost.T))\n",
    "\n",
    "    dcost_bh = np.dot(difference , weight_cost.T) * sigmoid_der(np.dot(X, weight1) + bias1)\n",
    "\n",
    "    # Update Weights ================\n",
    "\n",
    "    weight1 -= learning_rate * dcost_wh\n",
    "    bias1 -= learning_rate * dcost_bh.sum(axis=0)\n",
    "\n",
    "    weight_final -= learning_rate * weight_cost\n",
    "    bias_final -= learning_rate * bias_cost.sum(axis=0)\n",
    "\n",
    "    loss = np.sum(-one_hot_labels * np.log(output))\n",
    "    #print('Loss function value: ', loss)\n",
    "    error_cost.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(error_cost)\n",
    "plt.ylim(150,250);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
