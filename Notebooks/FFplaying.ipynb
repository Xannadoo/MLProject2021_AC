{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "## ITU Machine Learning Fall 2021\n",
    "### FF Neural Network From Scratch\n",
    "\n",
    "#### This notebook contains the code for the Neural Network part of our final project.\n",
    "\n",
    "#### Group AC\n",
    "Chrisanna Cornish <ccor@itu.dk> <br>\n",
    "Carl August Wismer <cwis@itu.dk><br>\n",
    "Danielle Marie Dequin <ddeq@itu.dk>\n",
    "\n",
    "Last Edited: 01/01/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = '../Data/df_train.csv'\n",
    "TEST = '../Data/df_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from exercise 5 to calculate the z-score\n",
    "z_score = lambda x : (x - np.mean(x, axis=0)) / np.std(x, axis=0)\n",
    "\n",
    "def conf_mat(y_hat, y_true):\n",
    "    '''Returns a confusion matrix'''\n",
    "    n = max(y_hat)+1\n",
    "    bingo = np.zeros([n,n])\n",
    "    for i in range(len(y_hat)):\n",
    "        bingo[y_true[i]][y_hat[i]] +=1\n",
    "    \n",
    "    return(bingo)\n",
    "\n",
    "def scores(y_hat, y_true, average = True):\n",
    "    '''For each class, returns recall, precision and f1'''\n",
    "    classes = list(np.unique(y_true))\n",
    "    conf = conf_mat(y_hat, y_true)\n",
    "    r = []\n",
    "    p = []\n",
    "    f = []\n",
    "    for c in classes:\n",
    "        recall = conf[c][c] / sum(conf[c])\n",
    "        precision = conf[c][c] / sum(conf[:, c])\n",
    "        f1 = 2*(precision*recall)/(precision + recall)\n",
    "        r.append(recall)\n",
    "        p.append(precision)\n",
    "        f.append(f1)\n",
    "    if average:\n",
    "        return sum(r)/len(r), sum(p)/len(p), sum(f)/len(f)\n",
    "    else:\n",
    "        return (r, p, f)\n",
    "    \n",
    "def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None, normalize=True):\n",
    "    \"\"\"\n",
    "    Function copied from exercise 7.\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.ylim([-0.5, cm.shape[0]-0.5])\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables and df loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'rainbow' # Colour theme\n",
    "\n",
    "df = pd.read_csv(TRAIN) # Training dataframe\n",
    "a = len(df)\n",
    "\n",
    "# ensures data is without order, random state fixed for reproducability, frac=1 gives the whole df back but shuffled\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "if a != len(df):\n",
    "    print('WARNING, DATA IS BEING LOST') # confirm still have the whole df\n",
    "\n",
    "attributes = list(df.columns)[:-1] # Creates list of column names for the dataframe without the class\n",
    "\n",
    "df[attributes] = z_score(df[attributes])\n",
    "\n",
    "X = df[attributes].copy() # Attributes\n",
    "y = df['type'].copy() # True values\n",
    "\n",
    "for i in range(len(y)):\n",
    "    if y[i] > 4:\n",
    "        y[i] = y[i] - 2\n",
    "    else:\n",
    "        y[i] = y[i] - 1\n",
    "        \n",
    "lb = preprocessing.LabelBinarizer()\n",
    "new_y = pd.DataFrame(lb.fit_transform(y))\n",
    "\n",
    "y_list = y.unique() # 'y' values\n",
    "\n",
    "\n",
    "df['type'] = y\n",
    "df[[0,1,2,3,4,5]] = new_y\n",
    "\n",
    "df.head(10)\n",
    "round(df[attributes].describe(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ideas from:\n",
    "\n",
    "https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6\n",
    "\n",
    "https://hackernoon.com/building-a-feedforward-neural-network-from-scratch-in-python-d3526457156b\n",
    "\n",
    "https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, x, y, neurons=10, lr=0.01):\n",
    "        '''\n",
    "        neurons --> neurons per hidden layer\n",
    "        activation --> choose activation function\n",
    "        '''\n",
    "        \n",
    "        self.input = x # All rows with the attributes (all X's)\n",
    "        #self.y = np.array(y) # True values\n",
    "        self.labels = y # true labels\n",
    "        self.rows = x.shape[0] # number of rows\n",
    "        self.class_count = len(y.unique()) # number of classes\n",
    "        self.y = np.zeros((self.rows, self.class_count)) # one hot labels\n",
    "        \n",
    "        for i in range(self.rows):\n",
    "            self.y[i, self.labels[i]] = 1\n",
    "        \n",
    "        self.output = np.zeros(self.y.shape) #\n",
    "        \n",
    "        self.neurons = neurons # number of neurons per label\n",
    "        np.random.seed(23)\n",
    "        \n",
    "        self.weight1 = np.random.rand(x.shape[1], neurons) # (attributes in X, number of neurons)\n",
    "        #self.weight2 = np.random.rand(neurons, neurons)\n",
    "        self.weight_final = np.random.rand(neurons, self.y.shape[1])\n",
    "\n",
    "        self.bias1 = np.random.randn(neurons)\n",
    "        #self.bias2 = np.random.randn(neurons)\n",
    "        self.bias_final = np.random.randn(self.class_count)\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.error_cost = []\n",
    "        self.accuracy = []\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def relu(self): # Rectified Linear Unit\n",
    "        return np.maximum(0, self.x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        if isinstance(x, pd.DataFrame):\n",
    "            exp = np.exp(x).to_numpy()\n",
    "        else: exp = np.exp(x)\n",
    "        probabilities = exp / np.sum(exp,axis=1, keepdims=True)\n",
    "        return probabilities \n",
    "    \n",
    "    def sigmoid_der(self, x):\n",
    "        \"\"\"Code borrowed from:\n",
    "        https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6\n",
    "        \"\"\"\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "   \n",
    "    def tanh_der(self, x):\n",
    "        return 1 - (self.tanh(x)**2)\n",
    "\n",
    "    def forwardpass(self, x):\n",
    "    \n",
    "        # 1st layer with Activation Function\n",
    "        #self.layer1 = self.sigmoid((x.dot(self.weight1) + self.bias1))\n",
    "        self.layer1 = self.tanh((x.dot(self.weight1) + self.bias1))\n",
    "        \n",
    "        #self.layer2 = self.sigmoid(self.layer1.dot(self.weight2) + self.bias2)\n",
    "        #self.layer2 = self.tanh((self.layer1.dot(self.weight2) + self.bias2)\n",
    "        #print(self.layer2.shape)\n",
    "        \n",
    "        # Final Output layer with softmax\n",
    "        self.output = self.softmax(self.layer1.dot(self.weight_final) + self.bias_final)\n",
    "        \n",
    "        return self.output\n",
    "   \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Back Propogation!!!!\n",
    "        \"\"\"\n",
    "        #print(self.output.shape, y.shape)\n",
    "        difference = self.output - y # Difference between forward output and true labels\n",
    "        weight_cost = np.dot(self.layer1.T, difference)\n",
    "        bias_cost = difference\n",
    "        \n",
    "        \n",
    "        #backwards_sig = self.sigmoid_der((np.dot(self.input, self.weight1) + self.bias1))\n",
    "        backwards_tanh = self.tanh_der((np.dot(x, self.weight1) + self.bias1))\n",
    "        \n",
    "        back_cost_w = np.dot(x.T, backwards_tanh * np.dot(difference, weight_cost.T))\n",
    "        back_cost_b = np.dot(difference, weight_cost.T) * backwards_tanh\n",
    "        \n",
    "        # Update weights and biases using learning rate\n",
    "        self.weight1 -= self.lr * back_cost_w\n",
    "        self.bias1 -= self.lr * back_cost_b.sum(axis=0)\n",
    "        \n",
    "        self.weight_final -= self.lr * weight_cost\n",
    "        self.bias_final -= self.lr * bias_cost.sum(axis=0)\n",
    "        \n",
    "        loss = -(1.0 / len(self.y) * np.sum(self.y * np.log(1e-15 + self.output))) #we could remove the tiny addition here, but with so little data, it makes little difference    \n",
    "        \n",
    "        y_pred = self.output.argmax(axis=1)\n",
    "        y_true = np.array(y).argmax(axis=1)\n",
    "        #print('pred: ', y_pred.shape, 'true: ', y_true.shape)\n",
    "        accuracy = (y_pred==y_true).mean()\n",
    "        \n",
    "        return loss, accuracy\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        \"\"\"Function that updates the output to the result of the forward pass, and \n",
    "        appends the error cost after backpropogation.\n",
    "        \"\"\"\n",
    "        self.output = self.forwardpass(X)\n",
    "        #print(self.output.shape, y.shape)\n",
    "        (cost, accuracy) = self.backprop(X, y)\n",
    "        self.error_cost.append(cost)\n",
    "        self.accuracy.append(accuracy)\n",
    "        \n",
    "    def predict(self, X, proba=False):\n",
    "        if proba:\n",
    "            return self.forwardpass(X)\n",
    "        else:\n",
    "            return self.forwardpass(X).argmax(axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Neural Network\n",
    "kitty = NeuralNetwork(X, y, neurons = 100)\n",
    "\n",
    "# Train for 1000 epochs\n",
    "for i in range(1500):\n",
    "    kitty.train(X, new_y)\n",
    "\n",
    "y_pred = kitty.predict(X, proba=True)\n",
    "print(f'Sum of all values for first record: {sum(y_pred[0])} \\\n",
    "    \\nFirst row label guesses: {y_pred[0]} \\\n",
    "    \\nPredicted Class: {np.argmax(y_pred[0])} \\\n",
    "    \\nTrue Class: {y[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(kitty.error_cost)\n",
    "plt.plot(kitty.accuracy)\n",
    "plt.legend(['cost', 'accuracy'])\n",
    "plt.ylim([0,1.1])\n",
    "plt.show() \n",
    "#This isn't pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n','#'*10,'Result for {} Data'.format('Test'), '#'*10, '\\n')\n",
    "\n",
    "y_pred = kitty.predict(X, proba= True)\n",
    "print('log_loss:   ', log_loss(y, y_pred, eps=1e-15))\n",
    "\n",
    "#y_true = one_hot_y.argmax(axis=1)\n",
    "y_pred = y_pred.argmax(axis=1)\n",
    "print('accuracy:   ',(y_pred==y).mean(), '\\n')\n",
    "\n",
    "target_names = ['class {}'.format(i+1) for i in range(6)]\n",
    "print(classification_report(y, y_pred, target_names=target_names,zero_division=0)) # Set 0 division to 0 as default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Confusion matrix\n",
    "confu = confusion_matrix(y, y_pred)\n",
    "\n",
    "plot_confusion_matrix(cm           = confu, \n",
    "                      normalize    = False,\n",
    "                      target_names = ['1', '2', '3', '4', '5', '6'],\n",
    "                      title        = \"Confusion Matrix: Test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#hyperparameters for tuning... this would probably be better automated somehow.\n",
    "neu = 100\n",
    "lr = 0.01\n",
    "epochs = 1500\n",
    "\n",
    "num_folds = 5\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=False)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "for train, test in kfold.split(X, y):\n",
    "    fold_x = X.iloc[train].copy().reset_index(drop=True)\n",
    "    fold_y = y.iloc[train].copy().reset_index(drop=True)\n",
    "    \n",
    "    luci = NeuralNetwork(fold_x, fold_y, neurons=neu, lr=lr)\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    new_f_y = pd.DataFrame(lb.fit_transform(fold_y))\n",
    "    # Fit data to model\n",
    "    for i in range(epochs): # number of epochs\n",
    "        luci.train(fold_x, new_f_y)\n",
    "    \n",
    "    y_pred = luci.predict(X.iloc[test])\n",
    "    print(sorted(pd.Series(fold_y).unique()), sorted(pd.Series(y_pred).unique()))\n",
    "    loss = log_loss(y.iloc[test], luci.predict(X.iloc[test], proba=True), eps=1e-15, labels=[0,1,2,3,4,5])\n",
    "\n",
    "    #y_true = new_y.argmax(axis=1)\n",
    "    accuracy = (y_pred==y.iloc[test]).mean()\n",
    "    \n",
    "    print(f'Score for fold {fold_no}: Loss of {loss}; Accuracy of {accuracy*100}%')\n",
    "    acc_per_fold.append(accuracy * 100)\n",
    "    loss_per_fold.append(loss)\n",
    "    \n",
    "    target_names = ['class {}'.format(i+1) for i in range(6)]\n",
    "    print(classification_report(y.iloc[test], y_pred, target_names=target_names,zero_division=0, labels=[0,1,2,3,4,5]))\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "print(f'\\n\\nAv_Accuracy: {round(np.array(acc_per_fold).mean(), 2)}%, Av_log_loss: {round(np.array(loss_per_fold).mean(), 4)}')\n",
    "print(f'Hyperparameters: Neurons: {neu}, learning rate: {lr}, Epochs: {epochs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=9)\n",
    "pca.fit(X)\n",
    "\n",
    "X_pca = pd.DataFrame(pca.transform(X))\n",
    "X_pca = z_score(X_pca)\n",
    "\n",
    "print(pca.explained_variance_ratio_, '\\n')\n",
    "pca_exp = pca.explained_variance_ratio_\n",
    "s = 0\n",
    "c = 1\n",
    "for i in pca_exp:\n",
    "    s += i\n",
    "    print(f'{c} components explain {round(100*s,2)}% of the data')\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=7) #99% of the data explained.\n",
    "pca.fit(X)\n",
    "\n",
    "X_pca = pd.DataFrame(pca.transform(X))\n",
    "X_pca = z_score(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Neural Network\n",
    "terri = NeuralNetwork(X_pca, y, neurons = 100, lr=0.005)\n",
    "\n",
    "# Train for 1000 epochs\n",
    "for i in range(1000):\n",
    "    terri.train(X_pca, new_y)\n",
    "\n",
    "y_pred = terri.predict(X_pca, proba = True)\n",
    "    \n",
    "print(f'Sum of all values for first record: {sum(y_pred[0])} \\\n",
    "    \\nFirst row label guesses: {y_pred[0]} \\\n",
    "    \\nPredicted Class: {np.argmax(y_pred[0])} \\\n",
    "    \\nTrue Class: {y[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(terri.error_cost)\n",
    "plt.plot(terri.accuracy)\n",
    "plt.legend(['cost', 'accuracy'])\n",
    "plt.ylim([0,1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n','#'*10,'Result for {} Data'.format('Test'), '#'*10, '\\n')\n",
    "\n",
    "y_pred = terri.predict(X_pca)\n",
    "print('log_loss:   ', log_loss(y, terri.predict(X_pca, proba=True), eps=1e-15))\n",
    "print('accuracy:   ',(y_pred==y).mean(), '\\n')\n",
    "\n",
    "target_names = ['class {}'.format(i+1) for i in range(6)]\n",
    "print(classification_report(y, y_pred, target_names=target_names,zero_division=0)) # Set 0 division to 0 as default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Confusion matrix\n",
    "confu = confusion_matrix(y, y_pred)\n",
    "\n",
    "plot_confusion_matrix(cm           = confu, \n",
    "                      normalize    = False,\n",
    "                      target_names = ['1', '2', '3', '4', '5', '6'],\n",
    "                      title        = \"Confusion Matrix: Test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters for tuning... this would probably be better automated somehow.\n",
    "neu = 100\n",
    "lr = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "num_folds = 5\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=False)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "for train, test in kfold.split(X_pca, y):\n",
    "    fold_x = X_pca.iloc[train].copy().reset_index(drop=True)\n",
    "    fold_y = y.iloc[train].copy().reset_index(drop=True)\n",
    "    pia = NeuralNetwork(fold_x, fold_y, neurons=neu, lr=lr)\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    new_f_y = pd.DataFrame(lb.fit_transform(fold_y))\n",
    "    # Fit data to model\n",
    "    for i in range(epochs): # number of epochs\n",
    "        pia.train(fold_x, new_f_y)\n",
    "    \n",
    "    y_pred = pia.predict(X_pca.iloc[test])\n",
    "    print(sorted(pd.Series(fold_y).unique()), sorted(pd.Series(y_pred).unique()))\n",
    "    loss = log_loss(y.iloc[test], pia.predict(X_pca.iloc[test], proba=True), eps=1e-15, labels=[0,1,2,3,4,5])\n",
    "\n",
    "    #y_true = new_y.argmax(axis=1)\n",
    "    accuracy = (y_pred==y.iloc[test]).mean()\n",
    "    \n",
    "    print(f'Score for fold {fold_no}: Loss of {loss}; Accuracy of {accuracy*100}%')\n",
    "    acc_per_fold.append(accuracy * 100)\n",
    "    loss_per_fold.append(loss)\n",
    "    \n",
    "    target_names = ['class {}'.format(i+1) for i in range(6)]\n",
    "    print(classification_report(y.iloc[test], y_pred, target_names=target_names,zero_division=0, labels=[0,1,2,3,4,5]))\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "print(f'\\n\\nAv_Accuracy: {round(np.array(acc_per_fold).mean(), 2)}%, Av_log_loss: {round(np.array(loss_per_fold).mean(), 4)}')\n",
    "print(f'Hyperparameters: Neurons: {neu}, learning rate: {lr}, Epochs: {epochs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set parameters for final test data here based on previous stuff.\n",
    "neu = 100\n",
    "lr = 0.01\n",
    "epochs = 1000\n",
    "use_PCA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''loads and predicts on the TEST data, \n",
    "uncomment this **last** once everything else is in place then don't change anything!'''\n",
    "\n",
    "'''df_test = pd.read_csv(TEST) #test dataframe\n",
    "\n",
    "attributes = list(df_test.columns)[:-1]\n",
    "\n",
    "X_test = df_test[attributes].copy() #attributes\n",
    "y_test = df_test['type'].copy() #true values\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] > 4:\n",
    "        y_test[i] = y_test[i] - 2\n",
    "    else:\n",
    "        y_test[i] = y_test[i] - 1\n",
    "\n",
    "\n",
    "if use_PCA:\n",
    "\n",
    "    pca = PCA(n_components=7) #99% of the data explained.\n",
    "    pca.fit(X)\n",
    "\n",
    "    X = pd.DataFrame(pca.transform(X))\n",
    "    X = z_score(X)\n",
    "\n",
    "    pca = PCA(n_components=7)\n",
    "    pca.fit(X_test)\n",
    "\n",
    "    X_test = pd.DataFrame(pca.transform(X_test))\n",
    "    X_test = z_score(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#Train the final model with the above parameters\n",
    "final = NeuralNetwork(X, y, neurons=neu, lr=lr) #set hyperparameters here\n",
    "for i in range(epochs): # number of epochs\n",
    "    final.train(X, new_y)\n",
    "\n",
    "#Predicts on the test data\n",
    "test_pred = final.predict(X_test)\n",
    "\n",
    "print('log_loss:   ', log_loss(y_test, final.predict(X_test, proba=True), eps=1e-15))\n",
    "print('accuracy:   ',(test_pred==y_test).mean(), '\\n')\n",
    "\n",
    "target_names = ['class {}'.format(i+1) for i in range(6)]\n",
    "print(classification_report(y_test, test_pred, target_names=target_names,zero_division=0))\n",
    "\n",
    "# Print Confusion matrix\n",
    "confu = confusion_matrix(y_test, test_pred)\n",
    "\n",
    "plot_confusion_matrix(cm           = confu, \n",
    "                      normalize    = False,\n",
    "                      target_names = ['1', '2', '3', '4', '5', '6'],\n",
    "                      title        = \"Confusion Matrix: Test data\")''';"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
