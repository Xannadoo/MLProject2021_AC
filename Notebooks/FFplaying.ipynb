{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = '../Data/df_train.csv'\n",
    "TEST = '../Data/df_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from exercise 5 to calculate the z-score\n",
    "z_score = lambda x : (x - np.mean(x, axis=0)) / np.std(x, axis=0)\n",
    "\n",
    "def conf_mat(y_hat, y_true):\n",
    "    '''Returns a confusion matrix'''\n",
    "    n = max(y_hat)+1\n",
    "    bingo = np.zeros([n,n])\n",
    "    for i in range(len(y_hat)):\n",
    "        bingo[y_true[i]][y_hat[i]] +=1\n",
    "    \n",
    "    return(bingo)\n",
    "\n",
    "def scores(y_hat, y_true, average = True):\n",
    "    '''For each class, returns recall, precision and f1'''\n",
    "    classes = list(np.unique(y_true))\n",
    "    conf = conf_mat(y_hat, y_true)\n",
    "    r = []\n",
    "    p = []\n",
    "    f = []\n",
    "    for c in classes:\n",
    "        recall = conf[c][c] / sum(conf[c])\n",
    "        precision = conf[c][c] / sum(conf[:, c])\n",
    "        f1 = 2*(precision*recall)/(precision + recall)\n",
    "        r.append(recall)\n",
    "        p.append(precision)\n",
    "        f.append(f1)\n",
    "    if average:\n",
    "        return sum(r)/len(r), sum(p)/len(p), sum(f)/len(f)\n",
    "    else:\n",
    "        return (r, p, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables and df loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>type</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.565103</td>\n",
       "      <td>-2.545930</td>\n",
       "      <td>-1.922352</td>\n",
       "      <td>-1.316840</td>\n",
       "      <td>0.751099</td>\n",
       "      <td>-0.854045</td>\n",
       "      <td>3.821565</td>\n",
       "      <td>-0.362309</td>\n",
       "      <td>-0.633117</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.673512</td>\n",
       "      <td>-0.488702</td>\n",
       "      <td>0.603378</td>\n",
       "      <td>0.208615</td>\n",
       "      <td>0.264239</td>\n",
       "      <td>0.219743</td>\n",
       "      <td>-0.587138</td>\n",
       "      <td>-0.362309</td>\n",
       "      <td>-0.633117</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.145715</td>\n",
       "      <td>0.092436</td>\n",
       "      <td>0.448165</td>\n",
       "      <td>0.386915</td>\n",
       "      <td>-0.876038</td>\n",
       "      <td>0.149331</td>\n",
       "      <td>-0.075888</td>\n",
       "      <td>-0.362309</td>\n",
       "      <td>0.291730</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.576697</td>\n",
       "      <td>1.603395</td>\n",
       "      <td>-1.922352</td>\n",
       "      <td>1.100115</td>\n",
       "      <td>0.622978</td>\n",
       "      <td>-0.854045</td>\n",
       "      <td>-0.427787</td>\n",
       "      <td>2.738558</td>\n",
       "      <td>-0.633117</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.617297</td>\n",
       "      <td>-0.023792</td>\n",
       "      <td>0.539882</td>\n",
       "      <td>0.168993</td>\n",
       "      <td>0.033621</td>\n",
       "      <td>0.325362</td>\n",
       "      <td>-0.560579</td>\n",
       "      <td>-0.362309</td>\n",
       "      <td>0.394491</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.595436</td>\n",
       "      <td>0.162173</td>\n",
       "      <td>0.596323</td>\n",
       "      <td>0.069937</td>\n",
       "      <td>-0.222621</td>\n",
       "      <td>0.272552</td>\n",
       "      <td>-0.640255</td>\n",
       "      <td>-0.362309</td>\n",
       "      <td>-0.633117</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.464267</td>\n",
       "      <td>-0.651420</td>\n",
       "      <td>0.603378</td>\n",
       "      <td>-0.247041</td>\n",
       "      <td>-0.017627</td>\n",
       "      <td>0.219743</td>\n",
       "      <td>-0.089167</td>\n",
       "      <td>-0.362309</td>\n",
       "      <td>-0.633117</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.643799</td>\n",
       "      <td>0.022699</td>\n",
       "      <td>-1.922352</td>\n",
       "      <td>0.287859</td>\n",
       "      <td>-0.517299</td>\n",
       "      <td>-0.290746</td>\n",
       "      <td>2.201499</td>\n",
       "      <td>-0.362309</td>\n",
       "      <td>-0.633117</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.841814</td>\n",
       "      <td>-1.302295</td>\n",
       "      <td>-1.922352</td>\n",
       "      <td>-0.861185</td>\n",
       "      <td>-3.156592</td>\n",
       "      <td>-0.642808</td>\n",
       "      <td>4.824146</td>\n",
       "      <td>-0.362309</td>\n",
       "      <td>1.833143</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.554836</td>\n",
       "      <td>-0.325983</td>\n",
       "      <td>0.511661</td>\n",
       "      <td>0.644459</td>\n",
       "      <td>-0.184185</td>\n",
       "      <td>0.202140</td>\n",
       "      <td>-0.361391</td>\n",
       "      <td>-0.362309</td>\n",
       "      <td>1.113817</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         RI        Na        Mg        Al        Si         K        Ca  \\\n",
       "0  2.565103 -2.545930 -1.922352 -1.316840  0.751099 -0.854045  3.821565   \n",
       "1 -0.673512 -0.488702  0.603378  0.208615  0.264239  0.219743 -0.587138   \n",
       "2 -0.145715  0.092436  0.448165  0.386915 -0.876038  0.149331 -0.075888   \n",
       "3 -0.576697  1.603395 -1.922352  1.100115  0.622978 -0.854045 -0.427787   \n",
       "4 -0.617297 -0.023792  0.539882  0.168993  0.033621  0.325362 -0.560579   \n",
       "5 -0.595436  0.162173  0.596323  0.069937 -0.222621  0.272552 -0.640255   \n",
       "6 -0.464267 -0.651420  0.603378 -0.247041 -0.017627  0.219743 -0.089167   \n",
       "7  1.643799  0.022699 -1.922352  0.287859 -0.517299 -0.290746  2.201499   \n",
       "8  4.841814 -1.302295 -1.922352 -0.861185 -3.156592 -0.642808  4.824146   \n",
       "9 -0.554836 -0.325983  0.511661  0.644459 -0.184185  0.202140 -0.361391   \n",
       "\n",
       "         Ba        Fe  type  0  1  2  3  4  5  \n",
       "0 -0.362309 -0.633117     1  0  1  0  0  0  0  \n",
       "1 -0.362309 -0.633117     1  0  1  0  0  0  0  \n",
       "2 -0.362309  0.291730     2  0  0  1  0  0  0  \n",
       "3  2.738558 -0.633117     5  0  0  0  0  0  1  \n",
       "4 -0.362309  0.394491     1  0  1  0  0  0  0  \n",
       "5 -0.362309 -0.633117     1  0  1  0  0  0  0  \n",
       "6 -0.362309 -0.633117     2  0  0  1  0  0  0  \n",
       "7 -0.362309 -0.633117     3  0  0  0  1  0  0  \n",
       "8 -0.362309  1.833143     1  0  1  0  0  0  0  \n",
       "9 -0.362309  1.113817     2  0  0  1  0  0  0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = 'rainbow' # Colour theme\n",
    "\n",
    "df = pd.read_csv(TRAIN) # Training dataframe\n",
    "a = len(df)\n",
    "\n",
    "#ensures data is without order, random state fixed for reproducability, frac=1 gives the whole df back but shuffled\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "if a != len(df):\n",
    "    print('WARNING, DATA IS BEING LOST')#confirm still have the whole df\n",
    "\n",
    "attributes = list(df.columns)[:-1] # Creates list of column names for the dataframe without the class\n",
    "\n",
    "df[attributes] = z_score(df[attributes])\n",
    "\n",
    "X = df[attributes].copy() # Attributes\n",
    "y = df['type'].copy() # True values\n",
    "\n",
    "for i in range(len(y)):\n",
    "    if y[i] > 4:\n",
    "        y[i] = y[i] - 2\n",
    "    else:\n",
    "        y[i] = y[i] - 1\n",
    "        \n",
    "lb = preprocessing.LabelBinarizer()\n",
    "new_y = pd.DataFrame(lb.fit_transform(y))\n",
    "\n",
    "y_list = y.unique() # 'y' values\n",
    "\n",
    "#round(df.describe(),2)\n",
    "df['type'] = y\n",
    "df[[0,1,2,3,4,5]] = new_y\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ideas from:\n",
    "\n",
    "https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6\n",
    "\n",
    "https://hackernoon.com/building-a-feedforward-neural-network-from-scratch-in-python-d3526457156b\n",
    "\n",
    "https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, x, y, bias=None, neurons=12):\n",
    "        '''\n",
    "        neurons --> neurons per hidden layer\n",
    "        activation --> choose activation function\n",
    "        '''\n",
    "        \n",
    "        self.input = x # All rows with the attributes (all X's)\n",
    "        self.y = np.array(y) # True values\n",
    "        self.output = np.zeros(self.y.shape)\n",
    "        \n",
    "        self.neurons = neurons\n",
    "        \n",
    "        self.weight1 = np.random.rand(x.shape[1], neurons) # (attributes in X, number of neurons)\n",
    "        self.weight2 = np.random.rand(neurons, neurons)\n",
    "        self.weight_final = np.random.rand(neurons, y.shape[1])\n",
    "\n",
    "        self.bias1 = bias[0]\n",
    "        self.bias2 = bias[1]\n",
    "        self.bias_final = bias[-1]\n",
    "    \n",
    "    def sigmoid(self, l):\n",
    "        return 1 / (1 + np.exp(-l))\n",
    "\n",
    "    def tanh(self):\n",
    "        return np.tanh(self.x)\n",
    "\n",
    "    def relu(self): #Rectified Linear Unit\n",
    "        return np.maximum(0, self.x)\n",
    "    \n",
    "    def softmax(self, vec):\n",
    "        exponential = np.exp(vec)\n",
    "        probabilities = exponential / np.sum(exponential)\n",
    "        return probabilities \n",
    "       \n",
    "    def forwardpass(self, x):\n",
    "        # activation_function(x * w_0 + bias_0) etc\n",
    "        \n",
    "        # Going through the 1st layer\n",
    "        self.layer1 = self.sigmoid((x.dot(self.weight1) + self.bias1))\n",
    "        \n",
    "        #self.layer2 = self.sigmoid(self.layer1.dot(self.weight2) + self.bias2)\n",
    "        #print(self.layer2.shape)\n",
    "        \n",
    "        # Final Output layer with softmax\n",
    "        self.output = self.softmax(self.layer1.dot(self.weight_final) + self.bias_final)\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    #https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6\n",
    "    #code borrowed from here\n",
    "    def sigmoid_derivative(self, p):\n",
    "        return p * (1 - p)\n",
    "\n",
    "    # calculate categorical cross entropy\n",
    "    def categorical_cross_entropy(self, X):\n",
    "        p = self.output.to_numpy() # predictions\n",
    "        sum_score = 0.0\n",
    "        for i in range(len(self.y)):\n",
    "            for j in range(len(self.y[i])):\n",
    "                sum_score += self.y[i][j] * np.log(1e-15 + p[i][j])\n",
    "        mean_sum_score = 1.0 / len(self.y) * sum_score\n",
    "        return -mean_sum_score\n",
    "    \n",
    "    def backprop(self):\n",
    "        # Update weights of last layer \n",
    "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * self.categorical_cross_entropy(self.output)))\n",
    "        \n",
    "        # Update weights of 1st layer\n",
    "        d_weights1 = np.dot(self.input.T, np.dot(2*(self.y - self.output) * self.sigmoid_derivative(self.output), self.weight_final.T)*self.sigmoid_derivative(self.layer1))\n",
    "        \n",
    "        print(self.input.shape, self.weight1.shape, self.y.shape, self.output.shape, self.weight_final.shape, self.layer1.shape)\n",
    "\n",
    "        # Update weights\n",
    "        self.weight1 += d_weights1\n",
    "        self.weight_final += d_weights2\n",
    "\n",
    "    def train(self, X, y):\n",
    "        self.output = self.forwardpass(X)\n",
    "        self.backprop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 12) (12, 12)\n",
      "(149, 9) (9, 12) (149, 6) (149, 6) (12, 6) (149, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias=np.array([0]*3)\n",
    "\n",
    "kitty = NeuralNetwork(X,new_y, bias = bias)\n",
    "print(kitty.weight1.shape, kitty.weight2.shape)\n",
    "#kitty.forwardpass(X)\n",
    "#kitty.backprop()\n",
    "kitty.train(X, new_y)\n",
    "sum(kitty.output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitty.outputlayer.iloc[0], new_y.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(self):\n",
    "    # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
    "    d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
    "    d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "class FFSN_MultiClass:\n",
    "  \n",
    "    def __init__(self, n_inputs, n_outputs, hidden_sizes=[3]):\n",
    "        self.nx = n_inputs\n",
    "        self.ny = n_outputs\n",
    "        self.nh = len(hidden_sizes)\n",
    "        self.sizes = [self.nx] + hidden_sizes + [self.ny] \n",
    "\n",
    "        self.W = {}\n",
    "        self.B = {}\n",
    "        for i in range(self.nh+1):\n",
    "            self.W[i+1] = np.random.randn(self.sizes[i], self.sizes[i+1])\n",
    "            self.B[i+1] = np.zeros((1, self.sizes[i+1]))\n",
    "      \n",
    "    def sigmoid(self, x):\n",
    "        return 1.0/(1.0 + np.exp(-x))\n",
    "  \n",
    "    def softmax(self, x):\n",
    "        exps = np.exp(x)\n",
    "        return exps / np.sum(exps)\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        self.A = {}\n",
    "        self.H = {}\n",
    "        self.H[0] = x.reshape(1, -1)\n",
    "        for i in range(self.nh):\n",
    "            self.A[i+1] = np.matmul(self.H[i], self.W[i+1]) + self.B[i+1]\n",
    "            self.H[i+1] = self.sigmoid(self.A[i+1])\n",
    "        self.A[self.nh+1] = np.matmul(self.H[self.nh], self.W[self.nh+1]) + self.B[self.nh+1]\n",
    "        self.H[self.nh+1] = self.softmax(self.A[self.nh+1])\n",
    "        return self.H[self.nh+1]\n",
    "  \n",
    "    def predict(self, X):\n",
    "        Y_pred = []\n",
    "        for x in X:\n",
    "            y_pred = self.forward_pass(x)\n",
    "            Y_pred.append(y_pred)\n",
    "        return np.array(Y_pred).squeeze()\n",
    " \n",
    "    def grad_sigmoid(self, x):\n",
    "        return x*(1-x) \n",
    "  \n",
    "    def cross_entropy(self,label,pred):\n",
    "        yl=np.multiply(pred,label)\n",
    "        yl=yl[yl!=0]\n",
    "        yl=-np.log(yl)\n",
    "        yl=np.mean(yl)\n",
    "        return yl\n",
    " \n",
    "    def grad(self, x, y):\n",
    "        self.forward_pass(x)\n",
    "        self.dW = {}\n",
    "        self.dB = {}\n",
    "        self.dH = {}\n",
    "        self.dA = {}\n",
    "        L = self.nh + 1\n",
    "        self.dA[L] = (self.H[L] - y)\n",
    "        for k in range(L, 0, -1):\n",
    "            self.dW[k] = np.matmul(self.H[k-1].T, self.dA[k])\n",
    "            self.dB[k] = self.dA[k]\n",
    "            self.dH[k-1] = np.matmul(self.dA[k], self.W[k].T)\n",
    "            self.dA[k-1] = np.multiply(self.dH[k-1], self.grad_sigmoid(self.H[k-1])) \n",
    "    \n",
    "    def fit(self, X, Y, epochs=100, initialize='True', learning_rate=0.01, display_loss=False):\n",
    "      \n",
    "        if display_loss:\n",
    "            loss = {}\n",
    "      \n",
    "        if initialize:\n",
    "            for i in range(self.nh+1):\n",
    "            self.W[i+1] = np.random.randn(self.sizes[i], self.sizes[i+1])\n",
    "            self.B[i+1] = np.zeros((1, self.sizes[i+1]))\n",
    "        \n",
    "        for epoch in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n",
    "            dW = {}\n",
    "            dB = {}\n",
    "            for i in range(self.nh+1):\n",
    "                dW[i+1] = np.zeros((self.sizes[i], self.sizes[i+1]))\n",
    "                dB[i+1] = np.zeros((1, self.sizes[i+1]))\n",
    "        for x, y in zip(X, Y):\n",
    "            self.grad(x, y)\n",
    "            for i in range(self.nh+1):\n",
    "                dW[i+1] += self.dW[i+1]\n",
    "                dB[i+1] += self.dB[i+1]\n",
    "                  \n",
    "        m = X.shape[1]\n",
    "        for i in range(self.nh+1):\n",
    "            self.W[i+1] -= learning_rate * (dW[i+1]/m)\n",
    "            self.B[i+1] -= learning_rate * (dB[i+1]/m)\n",
    "        \n",
    "        if display_loss:\n",
    "            Y_pred = self.predict(X) \n",
    "            loss[epoch] = self.cross_entropy(Y, Y_pred)\n",
    "    \n",
    "      if display_loss:\n",
    "          plt.plot(loss.values())\n",
    "          plt.xlabel('Epochs')\n",
    "          plt.ylabel('CE')\n",
    "          plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "\n",
    "milo = FFSN_MultiClass(9,6, [30])\n",
    "milo.fit(X, new_y,epochs=6000, learning_rate = 0.5, display_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = milo.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred'] = np.argmax(y_pred, axis=1)\n",
    "\n",
    "df[df['type'] == df['pred']].shape[0]\n",
    "df['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat(df['pred'], df['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
